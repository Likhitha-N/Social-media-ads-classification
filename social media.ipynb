{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraies\n",
    "#importing numpy\n",
    "import numpy as np\n",
    "#importing pandas\n",
    "import pandas as pd\n",
    "#import matplotlib for visualising the data\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "dataset=pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "Y = dataset.iloc[:, 4].values\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.memory_usage()\n",
    "#provides the memory in the bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count()\n",
    "#provides the total count in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Standard scalar-standardizes input data by removing the mean and scaling each variable to unit variance\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "#SVC-SUPPORT VECTOR CLASSIFIER-supervised learning\n",
    "#rbf-\n",
    "from sklearn.svm import SVC\n",
    "classify = SVC(kernel = 'rbf', random_state = 0)\n",
    "classify.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "Y_pred = classify.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "#confusion martix-a table that helps evaluate the performance of a machine learning model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conmatrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(conmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-Fold Cross Validation-a machine learning technique that evaluates predictive models.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score-a function that calculates a cross-validated accuracy score for each data point in a dataset.\n",
    "accurate = cross_val_score(estimator = classify, X = X_train, y = Y_train, cv = 10)\n",
    "print(accurate.mean())\n",
    "print(accurate.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "search_grid = GridSearchCV(estimator = classify,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "search_grid = search_grid.fit(X_train, Y_train)\n",
    "best_accuracy = search_grid.best_score_\n",
    "best_parameters = search_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', C = 1 ,gamma = 0.8)#previous 0.7\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, Y_set = X_train, Y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('cyan', 'magenta')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n",
    "                c = ListedColormap(('cyan', 'blue'))(i), label = j)\n",
    "plt.title('Kernel SVM (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Test set results\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, Y_set = X_test, Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('pink', 'orange')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n",
    "                c = ListedColormap(('pink', 'orange'))(i), label = j)\n",
    "plt.title('Kernel SVM (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.title(\"Product Purchased by people through social media marketing\")\n",
    "sns.histplot(x='Age',hue='Purchased',data=dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [257, 143]\n",
    "colors = ['orange', 'lightgreen']\n",
    "explode = [0, 0.001]\n",
    "labels = 'No','Yes'\n",
    "\n",
    "circle = plt.Circle((0, 0), 0.7, color = 'white')\n",
    "#to get default configration settings for matplotlib rumtime\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "plt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, pctdistance = 0.7, autopct = '%.2f%%')\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(circle)\n",
    "plt.title('Donut Chart', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of salary amongst the customers\n",
    "#distplot-shows the variation in a data's distribution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "sns.distplot(dataset['EstimatedSalary'], color = 'aqua')\n",
    "plt.title('Distrinution of Salary amongst the Customers', fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a scatter plot for age and purchased\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.scatter(dataset['Age'], dataset['EstimatedSalary'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column chart for looking at the relation of gender in purchased\n",
    "\n",
    "data = pd.crosstab(dataset['Purchased'], dataset['Gender'])\n",
    "data.div(data.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 10), color = ['pink', 'black'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "sns.violinplot(dataset['Purchased'])\n",
    "plt.title('Relation between Age and Purchasing Behaviour', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a pairplot for the data\n",
    "\n",
    "sns.pairplot(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a heatmap  for the data\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "sns.heatmap(dataset[['User ID', 'Age', 'EstimatedSalary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the optimum no. of clusters for the dataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "  km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "  km.fit(X)\n",
    "  wcss.append(km.inertia_)\n",
    "\n",
    "# plotting the results\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method', fontsize = 20)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making clusters\n",
    "kmeans = KMeans(n_clusters = 3, max_iter = 300,  init = 'k-means++', random_state = 0, n_init = 10)\n",
    "Y_means = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[Y_means == 0, 0], X[Y_means == 0, 1], s = 100, c = 'pink', label = 'general')\n",
    "plt.scatter(X[Y_means == 1, 0], X[Y_means == 1, 1], s = 100, c = 'yellow', label = 'miser')\n",
    "plt.scatter(X[Y_means == 2, 0], X[Y_means == 2, 1], s = 100, c = 'cyan', label = 'target')\n",
    "\n",
    "plt.title('Clusters of People according to their salary and purchasing behaviour',  fontsize = 20)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into dependent and independent sets\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "# checking the shape of the datasets\n",
    "print(\"Shape of X :\", X.shape)\n",
    "print('Shape of y :' ,y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
    "# getting the shapes of newly created datasets\n",
    "print(\"Shape of x_train : \", X_train.shape)\n",
    "print(\"Shape of x_test : \", X_test.shape)\n",
    "print(\"Shape of y_train : \", Y_train.shape)\n",
    "print(\"Shape of y_test : \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams['figure.figsize'] = (5 , 5)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "sns.heatmap(cm, annot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(Y_test, Y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('realtime_trending_searches.csv', index_col = [0])\n",
    "\n",
    "# Plot the trends\n",
    "def plotTrends(df, title):\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    sns.lineplot(data = df, dashes = False)\n",
    "    plt.title(f'{title[:40]}...', fontsize = 20)\n",
    "    plt.ylabel('Number of Searches', fontsize = 15)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "for i, title in enumerate(titles.title[:5]):\n",
    "    plotTrends(pd.read_csv(f'trend_{i + 1}.csv'), title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchResults = pd.read_csv('searchResults.csv', index_col = [0])\n",
    "searchResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular topics\n",
    "videoTopics = searchResults.topics.explode()\n",
    "topics = dict()\n",
    "\n",
    "# Count the number of times a topic appears\n",
    "for vidt in videoTopics:\n",
    "    if vidt is None or vidt is np.nan:\n",
    "        continue\n",
    "    vidt = vidt[1:-1].replace(\"'\", '').split(', ')\n",
    "    for t in vidt:\n",
    "        try:\n",
    "            topics[t] += 1\n",
    "        except:\n",
    "           topics[t] = 1\n",
    "topics = pd.Series(topics).sort_values(ascending = False)\n",
    "\n",
    "# Plot the topics\n",
    "plt.figure(figsize = (25, 8))\n",
    "plt.title('Most Popular Topics', fontsize = 20)\n",
    "plt.xlabel('Topics', fontsize = 20)\n",
    "sns.barplot(x = topics.index, y = topics.values, palette = 'Wistia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the views for each video\n",
    "videoViews = searchResults.views.explode()\n",
    "\n",
    "# Zip the views and topics\n",
    "videos = list(zip(videoViews, videoTopics))\n",
    "\n",
    "# Get the sum of total views for each topic\n",
    "viewsTopics = dict()\n",
    "for view, vidtp in videos:\n",
    "    if vidtp is None or vidtp is np.nan:\n",
    "        continue\n",
    "    vidtp = vidtp[1:-1].replace(\"'\", '').split(', ')\n",
    "    for t in vidtp:\n",
    "        try:\n",
    "            viewsTopics[t] += view\n",
    "        except:\n",
    "            viewsTopics[t] = view\n",
    "for k, v in viewsTopics.items():\n",
    "    viewsTopics[k] = v / topics[k]\n",
    "viewsTopics = pd.Series(viewsTopics).sort_values(ascending = False)\n",
    "\n",
    "# Plot the topics\n",
    "plt.figure(figsize = (25, 5))\n",
    "plt.title('Average Views per Topic', fontsize = 20)\n",
    "plt.xlabel('Topics', fontsize = 25)\n",
    "plt.ylabel('Average Views (in Millions)', fontsize = 25)\n",
    "sns.barplot(x = viewsTopics.index, y = viewsTopics.values, palette = 'Wistia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = pd.read_csv('top_posts.csv', index_col = [0])\n",
    "top_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_top_posts = pd.Series(top_posts['class'].explode().value_counts())\n",
    "# Plot the topics\n",
    "plt.figure(figsize = (25, 8))\n",
    "plt.title('Unique Top Posts', fontsize = 20)\n",
    "plt.xlabel('Topics', fontsize = 25)\n",
    "sns.barplot(x = unique_top_posts.index, y = unique_top_posts.values, palette = 'Wistia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes = top_posts['score'].explode()\n",
    "topics = top_posts['class'].explode()\n",
    "posts = list(zip(upvotes, topics))\n",
    "\n",
    "# Get the sum of total views for each topic\n",
    "upvotesTopics = dict()\n",
    "for upvote, topic in posts:\n",
    "    try:\n",
    "        upvotesTopics[topic] += upvote\n",
    "    except:\n",
    "        upvotesTopics[topic] = upvote\n",
    "\n",
    "# Get the average views for each topic\n",
    "for k, v in upvotesTopics.items():\n",
    "  upvotesTopics[k] = v / unique_top_posts[k]\n",
    "upvotesTopics = pd.Series(upvotesTopics).sort_values(ascending = False)\n",
    "\n",
    "# Plot the topics\n",
    "plt.figure(figsize = (25, 5))\n",
    "plt.title('Average Upvotes per Topic', fontsize = 20)\n",
    "plt.xlabel('Topics', fontsize = 25)\n",
    "plt.ylabel('Average Upvotes', fontsize = 25)\n",
    "sns.barplot(x = upvotesTopics.index, y = upvotesTopics.values, palette = 'Wistia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model=open('model.pkl','wb')\n",
    "pickle.dump(classifier,model)\n",
    "model.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
